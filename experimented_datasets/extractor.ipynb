{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f8b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to paginate. Finished collecting links.\n",
      "Total unique assessments found: 381\n",
      "âœ… Saved to shl_individual_assess.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "driver.get(\"https://www.shl.com/solutions/products/product-catalog/\")\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# STEP 1: Collect all assessment links with pagination\n",
    "assessment_links = set()\n",
    "\n",
    "while True:\n",
    "    # wait for the assessment links to load\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\")))\n",
    "    time.sleep(2)\n",
    "\n",
    "    # collect current page's assessment links\n",
    "    for elem in driver.find_elements(By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\"):\n",
    "        link = elem.get_attribute('href')\n",
    "        if link:\n",
    "            assessment_links.add(link)\n",
    "\n",
    "    # find all \"Next\" buttons on page\n",
    "    next_buttons = driver.find_elements(By.LINK_TEXT, \"Next\")\n",
    "\n",
    "    # if there are at least 2, the second is the Individual Test Solutions pagination\n",
    "    if len(next_buttons) >= 2:\n",
    "        btn = next_buttons[1]\n",
    "    # fallback: if only one Next exists, click it\n",
    "    elif len(next_buttons) == 1:\n",
    "        btn = next_buttons[0]\n",
    "    else:\n",
    "        print(\"No more pages to paginate. Finished collecting links.\")\n",
    "        break\n",
    "\n",
    "    # click and loop\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].click();\", btn)\n",
    "        time.sleep(2)\n",
    "    except Exception:\n",
    "        print(\"Failed to click Next. Exiting pagination loop.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total unique assessments found: {len(assessment_links)}\")\n",
    "def normalize_url(url):\n",
    "    return url.rstrip('/')\n",
    "\n",
    "driver.get(\"https://www.shl.com/solutions/products/product-catalog/\")\n",
    "adaptive_map = {}\n",
    "\n",
    "while True:\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"table tbody tr\")))\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            link_elem = row.find_element(By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\")\n",
    "            link = normalize_url(link_elem.get_attribute('href'))\n",
    "            adaptive = row.find_elements(By.CSS_SELECTOR, \"td\")[-2].text.strip()\n",
    "            adaptive_map[link] = adaptive\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    next_buttons = driver.find_elements(By.LINK_TEXT, \"Next\")\n",
    "    if len(next_buttons) >= 2:\n",
    "        btn = next_buttons[1]\n",
    "    elif len(next_buttons) == 1:\n",
    "        btn = next_buttons[0]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].click();\", btn)\n",
    "        time.sleep(2)\n",
    "    except Exception:\n",
    "        break\n",
    "\n",
    "print(f\"âœ… Collected Adaptive/IRT for {len(adaptive_map)} assessments.\")\n",
    "\n",
    "\n",
    "# STEP 2: Visit each assessment page and extract details\n",
    "data = []\n",
    "\n",
    "for link in assessment_links:\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # -------- Extract Title --------\n",
    "    try:\n",
    "        title = driver.find_element(By.TAG_NAME, 'h1').text.strip()\n",
    "    except:\n",
    "        title = None\n",
    "\n",
    "    # -------- Extract Metadata --------\n",
    "    metadata = {}\n",
    "    for h4 in driver.find_elements(By.CSS_SELECTOR, \"h4\"):\n",
    "        key = h4.text.strip().lower().rstrip(':')\n",
    "        try:\n",
    "            val = h4.find_element(By.XPATH, \"following-sibling::p[1]\").text.strip()\n",
    "        except:\n",
    "            val = None\n",
    "        metadata[key] = val\n",
    "\n",
    "    description = metadata.get('description')\n",
    "    if not description:\n",
    "        try:\n",
    "            description = driver.find_element(By.CSS_SELECTOR, 'div.description').text.strip()\n",
    "        except:\n",
    "            description = None\n",
    "\n",
    "    job_levels = metadata.get('job levels')\n",
    "    languages = metadata.get('languages') or metadata.get('report language availability')\n",
    "\n",
    "    raw_length = metadata.get('assessment length', '') or metadata.get('completion time', '')\n",
    "    match = re.search(r'(\\d+)', raw_length)\n",
    "    length = match.group(1) if match else None\n",
    "\n",
    "      # -------- âœ… Extract Test Type (Final, working method) --------\n",
    "    test_types = []\n",
    "    try:\n",
    "        p = driver.find_element(\n",
    "            By.XPATH,\n",
    "            \"//p[starts-with(normalize-space(text()), 'Test Type:')]\"\n",
    "        )\n",
    "        raw = p.text  # e.g. \"Test Type: A B P\"\n",
    "        test_types = re.findall(r'\\b[A-Z]\\b', raw)\n",
    "    except:\n",
    "        test_types = []\n",
    "        # -------- Extract Remote Testing (by looking for the green dot) --------\n",
    "    remote_testing = False\n",
    "    try:\n",
    "        # look for the <p> whose text starts with \"Remote Testing:\"\n",
    "        for p in driver.find_elements(By.CSS_SELECTOR, \"p.product-catalogue__small-text\"):\n",
    "            if p.text.strip().startswith(\"Remote Testing\"):\n",
    "                # if a green circle is present it will have class \"catalogue__circle -yes\"\n",
    "                dots = p.find_elements(By.CSS_SELECTOR, \"span.catalogue__circle.-yes\")\n",
    "                remote_testing = len(dots) > 0\n",
    "                break\n",
    "    except Exception:\n",
    "        remote_testing = False\n",
    "\n",
    "\n",
    "    # -------- Append Extracted Data --------\n",
    "    data.append({\n",
    "        'Title': title,\n",
    "        'Description': description,\n",
    "        'Job Levels': job_levels,\n",
    "        'Languages': languages,\n",
    "        'Assessment Length': length,\n",
    "        'Test Type': \", \".join(test_types),\n",
    "        'Remote Testing': remote_testing,\n",
    "        'Adaptive/IRT': adaptive_map.get(normalize_url(link)),  # ðŸ”¥ Normalize URL for match\n",
    "\n",
    "        'URL': link\n",
    "    })\n",
    "\n",
    "# STEP 3: Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('NEW_individual_assess.csv', index=False)\n",
    "print(\"âœ… Saved to shl_individual_assess.csv\")\n",
    "driver.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4a2753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to paginate. Finished collecting links.\n",
      "Total unique assessments found: 153\n",
      "âœ… Saved to NEW_pre_pack.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "driver.get(\"https://www.shl.com/solutions/products/product-catalog/\")\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# STEP 1: Collect all assessment links with pagination\n",
    "assessment_links = set()\n",
    "\n",
    "while True:\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\")))\n",
    "    time.sleep(2)\n",
    "    for elem in driver.find_elements(By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\"):\n",
    "        link = elem.get_attribute('href')\n",
    "        if link:\n",
    "            assessment_links.add(link)\n",
    "\n",
    "    try:\n",
    "        next_button = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, \"Next\")))\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"No more pages to paginate. Finished collecting links.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total unique assessments found: {len(assessment_links)}\")\n",
    "# First, go back to the main catalog page and collect Adaptive/IRT values from the table\n",
    "driver.get(\"https://www.shl.com/solutions/products/product-catalog/\")\n",
    "time.sleep(2)\n",
    "\n",
    "adaptive_map = {}\n",
    "wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"table\")))\n",
    "rows = driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\")\n",
    "\n",
    "for row in rows:\n",
    "    try:\n",
    "        link_elem = row.find_element(By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\")\n",
    "        link = link_elem.get_attribute('href')\n",
    "        adaptive = row.find_elements(By.CSS_SELECTOR, \"td\")[-2].text.strip()  # 2nd last td is Adaptive/IRT\n",
    "        adaptive_map[link] = adaptive\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# STEP 2: Visit each assessment page and extract details\n",
    "data = []\n",
    "\n",
    "for link in assessment_links:\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # -------- Extract Title --------\n",
    "    try:\n",
    "        title = driver.find_element(By.TAG_NAME, 'h1').text.strip()\n",
    "    except:\n",
    "        title = None\n",
    "\n",
    "    # -------- Extract Metadata --------\n",
    "    metadata = {}\n",
    "    for h4 in driver.find_elements(By.CSS_SELECTOR, \"h4\"):\n",
    "        key = h4.text.strip().lower().rstrip(':')\n",
    "        try:\n",
    "            val = h4.find_element(By.XPATH, \"following-sibling::p[1]\").text.strip()\n",
    "        except:\n",
    "            val = None\n",
    "        metadata[key] = val\n",
    "\n",
    "    description = metadata.get('description')\n",
    "    if not description:\n",
    "        try:\n",
    "            description = driver.find_element(By.CSS_SELECTOR, 'div.description').text.strip()\n",
    "        except:\n",
    "            description = None\n",
    "\n",
    "    job_levels = metadata.get('job levels')\n",
    "    languages = metadata.get('languages') or metadata.get('report language availability')\n",
    "\n",
    "    raw_length = metadata.get('assessment length', '') or metadata.get('completion time', '')\n",
    "    match = re.search(r'(\\d+)', raw_length)\n",
    "    length = match.group(1) if match else None\n",
    "\n",
    "    # -------- âœ… Extract Test Type (Final, working method) --------\n",
    "    test_types = []\n",
    "    try:\n",
    "        p = driver.find_element(\n",
    "            By.XPATH,\n",
    "            \"//p[starts-with(normalize-space(text()), 'Test Type:')]\"\n",
    "        )\n",
    "        raw = p.text  # e.g. \"Test Type: A B P\"\n",
    "        test_types = re.findall(r'\\b[A-Z]\\b', raw)\n",
    "    except:\n",
    "        test_types = []\n",
    "\n",
    "   # -------- Extract Remote Testing (by looking for the green dot) --------\n",
    "    remote_testing = False\n",
    "    try:\n",
    "        # look for the <p> whose text starts with \"Remote Testing:\"\n",
    "        for p in driver.find_elements(By.CSS_SELECTOR, \"p.product-catalogue__small-text\"):\n",
    "            if p.text.strip().startswith(\"Remote Testing\"):\n",
    "                # if a green circle is present it will have class \"catalogue__circle -yes\"\n",
    "                dots = p.find_elements(By.CSS_SELECTOR, \"span.catalogue__circle.-yes\")\n",
    "                remote_testing = len(dots) > 0\n",
    "                break\n",
    "    except Exception:\n",
    "        remote_testing = False\n",
    "\n",
    "\n",
    "    # --------  Append Extracted Data --------\n",
    "    data.append({\n",
    "        'Title': title,\n",
    "        'Description': description,\n",
    "        'Job Levels': job_levels,\n",
    "        'Languages': languages,\n",
    "        'Assessment Length': length,\n",
    "        'Test Type': \", \".join(test_types),\n",
    "        'Remote Testing': remote_testing,\n",
    "        'Adaptive/IRT': adaptive_map.get(link),  # <-- ðŸ”¥ New field here\n",
    "        'URL': link\n",
    "    })\n",
    "\n",
    "# STEP 3: Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('shl_pre_pack.csv', index=False)\n",
    "print(\"âœ… Saved to NEW_pre_pack.csv\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b98d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "ðŸ“„ Scraping current pageâ€¦\n",
      "â›” Reached last page; stopping.\n",
      "âœ… Extracted 141 rows â†’ shl_prepackaged_only.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Point to Preâ€‘packaged URL only\n",
    "URL = \"https://www.shl.com/products/product-catalog/?type=0\"\n",
    "\n",
    "# 2) Setup headless Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "driver.get(URL)\n",
    "time.sleep(4)  # give JS time to render the single table\n",
    "\n",
    "assessments = []\n",
    "\n",
    "while True:\n",
    "    print(\"ðŸ“„ Scraping current pageâ€¦\")\n",
    "\n",
    "    # 3) Extract rows from the Preâ€‘packaged table\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"div.custom__table-responsive > table\")\n",
    "    rows  = table.find_elements(By.CSS_SELECTOR, \"tbody tr\")\n",
    "    for row in rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(cells) < 4:\n",
    "            continue\n",
    "\n",
    "        link = cells[0].find_element(By.TAG_NAME, \"a\")\n",
    "        assessments.append({\n",
    "            \"Title\":         link.text.strip(),\n",
    "            \"URL\":           link.get_attribute(\"href\"),\n",
    "            \"RemoteTesting\": bool(cells[1].find_elements(By.CSS_SELECTOR, \".catalogue__circle.-yes\")),\n",
    "            \"Adaptive/IRT\":  bool(cells[2].find_elements(By.CSS_SELECTOR, \".catalogue__circle.-yes\")),\n",
    "            \"TestTypes\":     \", \".join([e.text for e in cells[3].find_elements(By.CSS_SELECTOR, \".product-catalogue__key\")])\n",
    "        })\n",
    "\n",
    "    # 4) Handle pagination via the global pager\n",
    "    try:\n",
    "        next_li = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"ul.pagination li.pagination__item.-arrow.-next\"\n",
    "        )\n",
    "        # If this li has \"-disabled\", we're done\n",
    "        if \"-disabled\" in next_li.get_attribute(\"class\"):\n",
    "            print(\"â›” Reached last page; stopping.\")\n",
    "            break\n",
    "\n",
    "        # Otherwise click its <a> child to advance\n",
    "        next_li.find_element(By.TAG_NAME, \"a\").click()\n",
    "        time.sleep(3)  # wait for new page to load\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        # Pager is missing or changed â€“ exit gracefully\n",
    "        print(\"âš ï¸ Cannot find next arrow; exiting pagination loop.\")\n",
    "        break\n",
    "    \n",
    "driver.quit()\n",
    "\n",
    "# 5) Save to CSV\n",
    "df = pd.DataFrame(assessments)\n",
    "df.to_csv(\"shl_prepackaged_only.csv\", index=False)\n",
    "print(f\"âœ… Extracted {len(df)} rows â†’ shl_prepackaged_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2a1551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged successfully and saved to 'merged_output.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\shl_prepackaged_no_duplicates_table.csv')\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\indiv_table_no_duplicates.csv')\n",
    "\n",
    "# Rename columns in df2 to match df1 (handle spacing and casing differences)\n",
    "#df2.columns = ['Title', 'URL', 'RemoteTesting', 'Adaptive/IRT', 'Test Types']\n",
    "\n",
    "# Concatenate both DataFrames row-wise\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Optional: Remove duplicate rows (if needed)\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Save to new CSV\n",
    "merged_df.to_csv('merged_table_output.csv', index=False)\n",
    "\n",
    "print(\"CSV files merged successfully and saved to 'merged_output.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39de399e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files merged successfully and saved to 'merged_output.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\shl_pre_pack.csv')\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\shl_individual_assess.csv')\n",
    "\n",
    "\n",
    "# Concatenate both DataFrames row-wise\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Optional: Remove duplicate rows (if needed)\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Save to new CSV\n",
    "merged_df.to_csv('merged_detail_output.csv', index=False)\n",
    "\n",
    "print(\"CSV files merged successfully and saved to 'merged_output.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a06834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved as 'first_file_with_adaptive_irt.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\merged_detail_output.csv')\n",
    "\n",
    "# Load the second CSV (only URL and Adaptive/IRT columns)\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\merged_table_output.csv')[['URL', 'Adaptive/IRT']]\n",
    "\n",
    "# Merge Adaptive/IRT into df1 based on URL\n",
    "merged_df = pd.merge(df1, df2, on='URL', how='left')\n",
    "\n",
    "# Save to a new CSV\n",
    "merged_df.to_csv('final.csv', index=False)\n",
    "\n",
    "print(\"Merged file saved as 'first_file_with_adaptive_irt.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66b7a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [Title, Description, Job Levels, Languages, Assessment Length, TestTypes, RemoteTesting, URL, Adaptive/IRT]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\final.csv\")\n",
    "\n",
    "# Find duplicate rows (excluding the index)\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "# Print duplicates\n",
    "print(\"Duplicate rows:\")\n",
    "print(duplicates)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a050e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned data saved to shl_assessments_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidhi\\AppData\\Local\\Temp\\ipykernel_19684\\1332143791.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load raw CSV\n",
    "df = pd.read_csv(\"final.csv\")\n",
    "\n",
    "# Rename columns to consistent lowercase, underscore-separated format\n",
    "df = df.rename(columns={\n",
    "    \"Title\": \"name\",\n",
    "    \"Description\": \"description\",\n",
    "    \"Job Levels\": \"job_levels\",\n",
    "    \"Languages\": \"languages\",\n",
    "    \"Assessment Length\": \"duration_minutes\",\n",
    "    \"TestTypes\": \"test_type\",\n",
    "    \"RemoteTesting\": \"is_remote\",\n",
    "    \"URL\": \"url\",\n",
    "    \"Adaptive/IRT\": \"is_adaptive\"\n",
    "})\n",
    "\n",
    "# Strip whitespace and handle missing values\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df[\"job_levels\"] = df[\"job_levels\"].fillna(\"\").str.split(\",\")\n",
    "df[\"languages\"] = df[\"languages\"].fillna(\"\").str.split(\",\")\n",
    "\n",
    "# Expand test_type abbreviations\n",
    "type_map = {\n",
    "    \"A\": \"Ability\",\n",
    "    \"B\": \"Behavioral\",\n",
    "    \"C\": \"Cognitive\",\n",
    "    \"P\": \"Personality\",\n",
    "    \"S\": \"Skills\"\n",
    "}\n",
    "def map_test_types(raw):\n",
    "    if pd.isna(raw): return []\n",
    "    types = [x.strip() for x in raw.split(\",\")]\n",
    "    return [type_map.get(t, t) for t in types]\n",
    "\n",
    "df[\"test_type\"] = df[\"test_type\"].apply(map_test_types)\n",
    "\n",
    "# Convert types\n",
    "df[\"duration_minutes\"] = pd.to_numeric(df[\"duration_minutes\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df[\"is_remote\"] = df[\"is_remote\"].astype(bool)\n",
    "df[\"is_adaptive\"] = df[\"is_adaptive\"].astype(bool)\n",
    "\n",
    "# Normalize and truncate descriptions\n",
    "def normalize_description(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[^\\w\\s.,!?;:()/-]\", \"\", text)\n",
    "    return ' '.join(text.split()[:300])  # limit to first 300 words\n",
    "\n",
    "df[\"description\"] = df[\"description\"].apply(normalize_description)\n",
    "\n",
    "# Drop rows missing key fields\n",
    "df = df.dropna(subset=[\"name\", \"description\", \"url\"])\n",
    "df = df.drop_duplicates(subset=[\"name\"])\n",
    "\n",
    "# Save cleaned version\n",
    "df.to_csv(\"shl_assessments_cleaned.csv\", index=False)\n",
    "print(\"âœ… Cleaned data saved to shl_assessments_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5bff590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset cleaned successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"shl_assessments_cleaned.csv\")\n",
    "\n",
    "# Helper function to clean list-like string columns\n",
    "def clean_list_column(series):\n",
    "    def safe_parse(val):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            return [x.strip().lower() for x in parsed if isinstance(x, str) and x.strip()]\n",
    "        except Exception:\n",
    "            return []\n",
    "    return series.apply(safe_parse)\n",
    "\n",
    "# Clean list-type columns\n",
    "list_columns = ['job_levels', 'languages', 'test_type']\n",
    "for col in list_columns:\n",
    "    df[col] = clean_list_column(df[col])\n",
    "\n",
    "# Normalize boolean columns\n",
    "df['remote_support'] = df['is_remote'].astype(str).str.upper().map({'TRUE': 'Yes', 'FALSE': 'No'})\n",
    "df['adaptive_support'] = df['is_adaptive'].astype(str).str.upper().map({'TRUE': 'Yes', 'FALSE': 'No'})\n",
    "\n",
    "# Ensure duration is numeric\n",
    "df['duration_minutes'] = pd.to_numeric(df['duration_minutes'], errors='coerce')\n",
    "\n",
    "# Optionally drop or warn about rows with NaN durations\n",
    "df = df.dropna(subset=['duration_minutes'])\n",
    "\n",
    "# Final cleaned dataframe\n",
    "cleaned_df = df.drop(columns=['is_remote', 'is_adaptive'])\n",
    "\n",
    "cleaned_df.to_csv(\"cleaned_dataset.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Dataset cleaned successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3b967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9e5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b35c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeccc61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40610bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb09e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4218416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31531f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fefc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb671a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9bfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Couldn't ensure Individual Test Solutions tab is selected: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//a[contains(text(), 'Individual Test Solutions')]\"}\n",
      "  (Session info: chrome=135.0.7049.116); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0053D363+60275]\n",
      "\tGetHandleVerifier [0x0053D3A4+60340]\n",
      "\t(No symbol) [0x003706F3]\n",
      "\t(No symbol) [0x003B8690]\n",
      "\t(No symbol) [0x003B8A2B]\n",
      "\t(No symbol) [0x00400EE2]\n",
      "\t(No symbol) [0x003DD0D4]\n",
      "\t(No symbol) [0x003FE6EB]\n",
      "\t(No symbol) [0x003DCE86]\n",
      "\t(No symbol) [0x003AC623]\n",
      "\t(No symbol) [0x003AD474]\n",
      "\tGetHandleVerifier [0x00788FE3+2467827]\n",
      "\tGetHandleVerifier [0x007845E6+2448886]\n",
      "\tGetHandleVerifier [0x0079F80C+2560028]\n",
      "\tGetHandleVerifier [0x00553DF5+153093]\n",
      "\tGetHandleVerifier [0x0055A3BD+179149]\n",
      "\tGetHandleVerifier [0x00544BB8+91080]\n",
      "\tGetHandleVerifier [0x00544D60+91504]\n",
      "\tGetHandleVerifier [0x0052FA10+4640]\n",
      "\tBaseThreadInitThunk [0x74935D49+25]\n",
      "\tRtlInitializeExceptionChain [0x76EDCFFB+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x76EDCF81+561]\n",
      "\n",
      "Scraping table page 1/12â€¦\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    104\u001b[39m     driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33marguments[0].click();\u001b[39m\u001b[33m\"\u001b[39m, next_btns[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâ†’ Collected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_entries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m table rows total.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Phase 2: enrich each with detail-page metadata\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run in headless mode\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Navigate to the SHL product catalog page\n",
    "driver.get(\"https://www.shl.com/products/product-catalog/?start=0&type=1&type=1\")\n",
    "time.sleep(5)  # Wait for the page to load completely\n",
    "\n",
    "# Initialize a list to store the extracted data\n",
    "assessments = []\n",
    "\n",
    "while True:\n",
    "    print(\"ðŸ“„ Scraping current page...\")\n",
    "\n",
    "    # Locate all table rows in the catalog\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            # Extract all table data cells in the row\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            if len(cells) < 4:\n",
    "                continue  # Skip rows that don't have enough columns\n",
    "\n",
    "            # Extract assessment title and URL\n",
    "            title_element = cells[0].find_element(By.TAG_NAME, \"a\")\n",
    "            title = title_element.text.strip()\n",
    "            url = title_element.get_attribute(\"href\")\n",
    "\n",
    "            # Determine if Remote Testing is available\n",
    "            remote_testing = bool(cells[1].find_elements(By.CLASS_NAME, \"catalogue__circle\"))\n",
    "\n",
    "            # Determine if Adaptive/IRT is available\n",
    "            adaptive_irt = bool(cells[2].find_elements(By.CLASS_NAME, \"catalogue__circle\"))\n",
    "\n",
    "            # Extract Test Types\n",
    "            test_type_elements = cells[3].find_elements(By.CLASS_NAME, \"product-catalogue__key\")\n",
    "            test_types = [elem.text.strip() for elem in test_type_elements]\n",
    "\n",
    "            # Append the extracted information to the list\n",
    "            assessments.append({\n",
    "                \"Title\": title,\n",
    "                \"URL\": url,\n",
    "                \"Remote Testing\": remote_testing,\n",
    "                \"Adaptive/IRT\": adaptive_irt,\n",
    "                \"Test Types\": \", \".join(test_types)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Try to find and click the \"Next\" button\n",
    "    try:\n",
    "        next_buttons = driver.find_elements(By.LINK_TEXT, \"Next\")\n",
    "        if not next_buttons:\n",
    "            print(\"â›” No more pages to paginate. Exiting.\")\n",
    "            break\n",
    "\n",
    "        next_button = next_buttons[-1]  # Use the last Next button\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(3)  # Wait for the next page to load\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to click Next button: {e}\")\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Convert the list to a DataFrame and save to CSV\n",
    "df = pd.DataFrame(assessments)\n",
    "df.to_csv(\"shl_pre_pack_table.csv\", index=False)\n",
    "print(\"âœ… Data extraction complete. Saved to 'shl_assessments_with_pagination.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
