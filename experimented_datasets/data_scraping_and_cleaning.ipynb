{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb1b925",
   "metadata": {},
   "source": [
    "Web Scraping: Adaptive and Pre-packaged Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72fbc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "# ---------------- Scraping Individual (Adaptive/IRT) Assessments ----------------\n",
    "assessment_links = set()\n",
    "\n",
    "# STEP 1: Collect all assessment links with pagination\n",
    "while True:\n",
    "    # wait for the assessment links to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\")))\n",
    "    time.sleep(2)\n",
    "\n",
    "    # collect current page's assessment links\n",
    "    for elem in driver.find_elements(By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\"):\n",
    "        link = elem.get_attribute('href')\n",
    "        if link:\n",
    "            assessment_links.add(link)\n",
    "\n",
    "    # find all \"Next\" buttons on page\n",
    "    next_buttons = driver.find_elements(By.LINK_TEXT, \"Next\")\n",
    "\n",
    "    # if there are at least 2, the second is the Individual Test Solutions pagination\n",
    "    if len(next_buttons) >= 2:\n",
    "        if next_buttons[1].get_attribute(\"aria-disabled\") == \"true\":\n",
    "            print(\"‚õî Reached last page; stopping.\")\n",
    "            break\n",
    "        else:\n",
    "            # click next page\n",
    "            next_buttons[1].click()\n",
    "            time.sleep(3)  # wait for new page to load\n",
    "    else:\n",
    "        print(\"No more pages to paginate. Finished collecting links.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total unique assessments found: {len(assessment_links)}\")\n",
    "\n",
    "# STEP 2: Visit each assessment page and extract details\n",
    "data = []\n",
    "\n",
    "for link in assessment_links:\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # -------- Extract Title --------\n",
    "    try:\n",
    "        title_elem = driver.find_element(By.CSS_SELECTOR, \"h1.main-title\")\n",
    "        title = title_elem.text.strip()\n",
    "    except:\n",
    "        title = None\n",
    "\n",
    "    # -------- Extract Description --------\n",
    "    try:\n",
    "        desc_elem = driver.find_element(By.CSS_SELECTOR, \"div.prose\")\n",
    "        description = desc_elem.text.strip()\n",
    "    except:\n",
    "        description = None\n",
    "\n",
    "    # -------- Extract Job Levels --------\n",
    "    job_levels = []\n",
    "    try:\n",
    "        levels = driver.find_elements(By.CSS_SELECTOR, \"div.job-levels span\")\n",
    "        job_levels = [lvl.text.strip() for lvl in levels if lvl.text.strip()]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # -------- Extract Languages --------\n",
    "    languages = []\n",
    "    try:\n",
    "        langs = driver.find_elements(By.CSS_SELECTOR, \"ul.languages li\")\n",
    "        languages = [lang.text.strip() for lang in langs if lang.text.strip()]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # -------- Extract Assessment Length --------\n",
    "    try:\n",
    "        length_elem = driver.find_element(By.CSS_SELECTOR, \"p.product-catalogue__small-text span.catalogue__text\")\n",
    "        length = length_elem.text.strip()\n",
    "    except:\n",
    "        length = None\n",
    "\n",
    "    # -------- Extract Test Types (A, B, P, etc) --------\n",
    "    try:\n",
    "        p = driver.find_element(By.XPATH, \"//p[normalize-space().startswith('Test Type')]\")\n",
    "        raw = p.text  # e.g. \"Test Type: A B P\"\n",
    "        test_types = re.findall(r'\\b[A-Z]\\b', raw)\n",
    "    except:\n",
    "        test_types = []\n",
    "\n",
    "    # -------- Extract Remote Testing (green dot) --------\n",
    "    remote_testing = False\n",
    "    try:\n",
    "        for p in driver.find_elements(By.CSS_SELECTOR, \"p.product-catalogue__small-text\"):\n",
    "            if p.text.strip().startswith(\"Remote Testing\"):\n",
    "                dots = p.find_elements(By.CSS_SELECTOR, \"span.catalogue__circle.-yes\")\n",
    "                remote_testing = len(dots) > 0\n",
    "                break\n",
    "    except Exception:\n",
    "        remote_testing = False\n",
    "\n",
    "    # --------  Append Extracted Data --------\n",
    "    data.append({\n",
    "        'Title': title,\n",
    "        'Description': description,\n",
    "        'Job Levels': job_levels,\n",
    "        'Languages': languages,\n",
    "        'Assessment Length': length,\n",
    "        'Test Type': \", \".join(test_types),\n",
    "        'Remote Testing': remote_testing,\n",
    "        'Adaptive/IRT': adaptive_map.get(link),  # <-- üî• New field here\n",
    "        'URL': link\n",
    "    })\n",
    "\n",
    "# STEP 3: Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('shl_pre_pack.csv', index=False)\n",
    "print(\"‚úÖ Saved to NEW_pre_pack.csv\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5d7e3",
   "metadata": {},
   "source": [
    "CSV Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Scraping Pre-packaged Assessments (Global Pre-packaged Table) ----------------\n",
    "# 1) Point to Pre-packaged URL only\n",
    "URL = \"https://www.shl.com/products/product-catalog/?type=0\"\n",
    "\n",
    "# 2) Setup headless Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "driver.get(URL)\n",
    "time.sleep(2)\n",
    "\n",
    "# 3) Initialize list to store assessments\n",
    "assessments = []\n",
    "\n",
    "while True:\n",
    "    print(\"üìÑ Scraping current page‚Ä¶\")\n",
    "\n",
    "    # Extract rows from the Pre-packaged table\n",
    "    table = driver.find_element(By.CSS_SELECTOR, \"div.custom__table-responsive > table\")\n",
    "    rows = table.find_elements(By.CSS_SELECTOR, \"tbody tr\")\n",
    "    for row in rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(cells) < 4:\n",
    "            continue\n",
    "\n",
    "        link = cells[0].find_element(By.TAG_NAME, \"a\")\n",
    "        assessments.append({\n",
    "            \"Title\":         link.text.strip(),\n",
    "            \"URL\":           link.get_attribute(\"href\"),\n",
    "            \"RemoteTesting\": bool(cells[1].find_elements(By.CSS_SELECTOR, \".catalogue__circle.-yes\")),\n",
    "            \"Adaptive/IRT\":  bool(cells[2].find_elements(By.CSS_SELECTOR, \".catalogue__circle.-yes\")),\n",
    "            \"TestTypes\":     \", \".join([e.text for e in cells[3].find_elements(By.CSS_SELECTOR, \".product-catalogue__key\")])\n",
    "        })\n",
    "\n",
    "    # 4) Handle pagination via the global pager\n",
    "    try:\n",
    "        next_li = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"ul.pagination li.pagination__item.-arrow.-next\"\n",
    "        )\n",
    "        # If this li has \"-disabled\", we‚Äôre done\n",
    "        if \"-disabled\" in next_li.get_attribute(\"class\"):\n",
    "            print(\"‚õî Reached last page; stopping.\")\n",
    "            break\n",
    "\n",
    "        # Otherwise click its <a> child to advance\n",
    "        next_li.find_element(By.TAG_NAME, \"a\").click()\n",
    "        time.sleep(3)  # wait for new page to load\n",
    "\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Cannot find next arrow; exiting pagination loop.\")\n",
    "        break\n",
    "\n",
    "# 5) Save to CSV\n",
    "df = pd.DataFrame(assessments)\n",
    "df.to_csv(\"shl_prepackaged_only.csv\", index=False)\n",
    "print(f\"‚úÖ Extracted {len(df)} rows ‚Üí shl_prepackaged_only.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge Step 1: Load prepackaged and individual table CSVs\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\shl_prepackaged_no_duplicates_table.csv')\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\indiv_table_no_duplicates.csv')\n",
    "\n",
    "# Concatenate both DataFrames row-wise\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Remove duplicate rows (if needed)\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Save merged table (prepackaged + individual) to new CSV\n",
    "merged_df.to_csv('merged_table_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge Step 2: Load the two CSV files with scraped data\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\shl_pre_pack.csv')\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\shl_individual_assess.csv')\n",
    "\n",
    "# Concatenate both DataFrames row-wise\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Optional: Remove duplicate rows (if needed)\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Save to new CSV\n",
    "merged_df.to_csv('merged_detail_output.csv', index=False)\n",
    "print(\"CSV files merged successfully and saved to 'merged_output.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge Step 3: Load the merged detail and table CSVs\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\merged_detail_output.csv')\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\vidhi\\\\Downloads\\\\my_shl\\\\merged_table_output.csv')[['URL', 'Adaptive/IRT']]\n",
    "\n",
    "# Merge Adaptive/IRT into df1 based on URL\n",
    "merged_df = pd.merge(df1, df2, on='URL', how='left')\n",
    "\n",
    "# Save the final merged CSV\n",
    "merged_df.to_csv('final.csv', index=False)\n",
    "print(\"Merged file saved as 'first_file_with_adaptive_irt.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8f6b0",
   "metadata": {},
   "source": [
    "Data Cleaning and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Scraping complete. Starting data cleaning...\")\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load the merged dataset\n",
    "df = pd.read_csv(\"shl_assessments_cleaned.csv\")\n",
    "\n",
    "# Helper function to clean list-like string columns\n",
    "def clean_list_column(series):\n",
    "    def safe_parse(val):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            return [x.strip().lower() for x in parsed if isinstance(x, str) and x.strip()]\n",
    "        except Exception:\n",
    "            return []\n",
    "    return series.apply(safe_parse)\n",
    "\n",
    "# Clean list-type columns\n",
    "list_columns = ['job_levels', 'languages', 'test_type']\n",
    "for col in list_columns:\n",
    "    df[col] = clean_list_column(df[col])\n",
    "\n",
    "# Normalize boolean columns\n",
    "df['remote_support'] = df['is_remote'].astype(str).str.upper().map({'TRUE': 'Yes', 'FALSE': 'No'})\n",
    "df['adaptive_support'] = df['is_adaptive'].astype(str).str.upper().map({'TRUE': 'Yes', 'FALSE': 'No'})\n",
    "\n",
    "# Ensure duration is numeric\n",
    "df['duration_minutes'] = pd.to_numeric(df['duration_minutes'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing durations\n",
    "df = df.dropna(subset=['duration_minutes'])\n",
    "\n",
    "# Final cleaned dataframe\n",
    "cleaned_df = df.drop(columns=['is_remote', 'is_adaptive'])\n",
    "cleaned_df.to_csv(\"cleaned_dataset.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset cleaned successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# STEP 1: Collect all assessment links with pagination\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "driver.get(\"https://www.shl.com/solutions/products/product-catalog/\")\n",
    "time.sleep(2)\n",
    "\n",
    "assessment_links = set()\n",
    "while True:\n",
    "    # wait for the assessment links to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\")))\n",
    "    time.sleep(2)\n",
    "\n",
    "    # collect current page's assessment links\n",
    "    for elem in driver.find_elements(By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\"):\n",
    "        link = elem.get_attribute('href')\n",
    "        if link:\n",
    "            assessment_links.add(link)\n",
    "\n",
    "    # find all \"Next\" buttons on page\n",
    "    next_buttons = driver.find_elements(By.LINK_TEXT, \"Next\")\n",
    "\n",
    "    # if there are at least 2, the second is the Individual Test Solutions pagination\n",
    "    if len(next_buttons) >= 2:\n",
    "        if next_buttons[1].get_attribute(\"aria-disabled\") == \"true\":\n",
    "            print(\"‚õî Reached last page; stopping.\")\n",
    "            break\n",
    "        else:\n",
    "            next_buttons[1].click()\n",
    "            time.sleep(3)\n",
    "    else:\n",
    "        print(\"No more pages to paginate. Finished collecting links.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total unique assessments found: {len(assessment_links)}\")\n",
    "\n",
    "# STEP 2: Visit each assessment page and extract details\n",
    "data = []\n",
    "\n",
    "for link in assessment_links:\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Extract details similarly as above\n",
    "    try:\n",
    "        title = driver.find_element(By.CSS_SELECTOR, \"h1.main-title\").text.strip()\n",
    "    except:\n",
    "        title = None\n",
    "    try:\n",
    "        description = driver.find_element(By.CSS_SELECTOR, \"div.prose\").text.strip()\n",
    "    except:\n",
    "        description = None\n",
    "    job_levels = []\n",
    "    try:\n",
    "        levels = driver.find_elements(By.CSS_SELECTOR, \"div.job-levels span\")\n",
    "        job_levels = [lvl.text.strip() for lvl in levels if lvl.text.strip()]\n",
    "    except:\n",
    "        pass\n",
    "    languages = []\n",
    "    try:\n",
    "        langs = driver.find_elements(By.CSS_SELECTOR, \"ul.languages li\")\n",
    "        languages = [lang.text.strip() for lang in langs if lang.text.strip()]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        length = driver.find_element(By.CSS_SELECTOR, \"p.product-catalogue__small-text span.catalogue__text\").text.strip()\n",
    "    except:\n",
    "        length = None\n",
    "    try:\n",
    "        p = driver.find_element(By.XPATH, \"//p[normalize-space().startswith('Test Type')]\")\n",
    "        test_types = re.findall(r'\\b[A-Z]\\b', p.text)\n",
    "    except:\n",
    "        test_types = []\n",
    "    remote_testing = False\n",
    "    try:\n",
    "        for p in driver.find_elements(By.CSS_SELECTOR, \"p.product-catalogue__small-text\"):\n",
    "            if p.text.strip().startswith(\"Remote Testing\"):\n",
    "                dots = p.find_elements(By.CSS_SELECTOR, \"span.catalogue__circle.-yes\")\n",
    "                remote_testing = len(dots) > 0\n",
    "                break\n",
    "    except:\n",
    "        remote_testing = False\n",
    "\n",
    "    data.append({\n",
    "        \"Title\": title,\n",
    "        \"Description\": description,\n",
    "        \"Job Levels\": job_levels,\n",
    "        \"Languages\": languages,\n",
    "        \"Duration\": length,\n",
    "        \"TestTypes\": \", \".join(test_types),\n",
    "        \"RemoteTesting\": remote_testing,\n",
    "        \"URL\": link\n",
    "    })\n",
    "\n",
    "# STEP 3: Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('shl_individual_assess.csv', index=False)\n",
    "print(\"‚úÖ Saved to shl_individual_assess.csv\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# STEP 1: Collect all assessment links with pagination (second scraper)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "driver.get(\"https://www.shl.com/solutions/products/product-catalog/\")\n",
    "time.sleep(2)\n",
    "\n",
    "assessment_links = set()\n",
    "while True:\n",
    "    # wait for the assessment links to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\")))\n",
    "    time.sleep(2)\n",
    "\n",
    "    # collect current page's assessment links\n",
    "    for elem in driver.find_elements(By.CSS_SELECTOR, \"a[href*='/product-catalog/view/']\"):\n",
    "        link = elem.get_attribute('href')\n",
    "        if link:\n",
    "            assessment_links.add(link)\n",
    "\n",
    "    # find all \"Next\" buttons on page\n",
    "    next_buttons = driver.find_elements(By.LINK_TEXT, \"Next\")\n",
    "\n",
    "    # if there are at least 2, the second is the Individual Test Solutions pagination\n",
    "    if len(next_buttons) >= 2:\n",
    "        if next_buttons[1].get_attribute(\"aria-disabled\") == \"true\":\n",
    "            print(\"‚õî Reached last page; stopping.\")\n",
    "            break\n",
    "        else:\n",
    "            next_buttons[1].click()\n",
    "            time.sleep(3)\n",
    "    else:\n",
    "        print(\"No more pages to paginate. Finished collecting links.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total unique assessments found: {len(assessment_links)}\")\n",
    "\n",
    "# STEP 2: Visit each assessment page and extract details\n",
    "data = []\n",
    "for link in assessment_links:\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    title = driver.find_element(By.CSS_SELECTOR, \"h1.main-title\").text.strip() if driver.find_elements(By.CSS_SELECTOR, \"h1.main-title\") else None\n",
    "    description = driver.find_element(By.CSS_SELECTOR, \"div.prose\").text.strip() if driver.find_elements(By.CSS_SELECTOR, \"div.prose\") else None\n",
    "\n",
    "    job_levels = [lvl.text.strip() for lvl in driver.find_elements(By.CSS_SELECTOR, \"div.job-levels span\") if lvl.text.strip()]\n",
    "\n",
    "    languages = [lang.text.strip() for lang in driver.find_elements(By.CSS_SELECTOR, \"ul.languages li\") if lang.text.strip()]\n",
    "\n",
    "    length = driver.find_element(By.CSS_SELECTOR, \"p.product-catalogue__small-text span.catalogue__text\").text.strip() if driver.find_elements(By.CSS_SELECTOR, \"p.product-catalogue__small-text span.catalogue__text\") else None\n",
    "\n",
    "    try:\n",
    "        raw = driver.find_element(By.XPATH, \"//p[normalize-space().startswith('Test Type')]\").text\n",
    "        test_types = re.findall(r'\\b[A-Z]\\b', raw)\n",
    "    except:\n",
    "        test_types = []\n",
    "\n",
    "    remote_testing = False\n",
    "    try:\n",
    "        for p in driver.find_elements(By.CSS_SELECTOR, \"p.product-catalogue__small-text\"):\n",
    "            if p.text.strip().startswith(\"Remote Testing\"):\n",
    "                dots = p.find_elements(By.CSS_SELECTOR, \"span.catalogue__circle.-yes\")\n",
    "                remote_testing = len(dots) > 0\n",
    "                break\n",
    "    except:\n",
    "        remote_testing = False\n",
    "\n",
    "    data.append({\n",
    "        'Title': title,\n",
    "        'Description': description,\n",
    "        'Job Levels': job_levels,\n",
    "        'Languages': languages,\n",
    "        'Assessment Length': length,\n",
    "        'TestTypes': \", \".join(test_types),\n",
    "        'RemoteTesting': remote_testing,\n",
    "        'Adaptive/IRT': None,  # no data in this loop\n",
    "        'URL': link\n",
    "    })\n",
    "\n",
    "# STEP 3: Save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('shl_pre_pack.csv', index=False)\n",
    "print(\"‚úÖ Saved to shl_pre_pack.csv\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc79bc",
   "metadata": {},
   "source": [
    "Combined Text Column Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_combined_text_column(csv_path, output_path=\"assessments_with_combined_text.csv\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Function to clean the title by removing numbers, brackets, and special characters\n",
    "    def clean_title(text):\n",
    "        text = re.sub(r\"\\d+\", \"\", text)  # remove numbers\n",
    "        text = re.sub(r\"\\[.*?\\]|\\(.*?\\)|\\{.*?\\}\", \"\", text)  # remove bracketed text\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)  # remove special characters\n",
    "        return text.strip()\n",
    "\n",
    "    # Create combined text column\n",
    "    df['combined_text'] = df['description'].fillna(\"\") + \" \" + df['Title'].fillna(\"\")\n",
    "    df['combined_text'] = df['combined_text'].apply(clean_title)\n",
    "    df = df.dropna(subset=['combined_text'])\n",
    "\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Combined text and cleaned title saved to {output_path}\")\n",
    "    return df\n",
    "\n",
    "# Example run\n",
    "create_combined_text_column(\"metadata_cleaned.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
